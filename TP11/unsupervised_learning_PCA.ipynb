{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIhlddTRcwQ3"
   },
   "source": [
    "# EE-411 TP session 11\n",
    "\n",
    "## Part 1: Principal Component Analysis\n",
    "\n",
    "In this set of exercises we will play a little bit with PCA, first introducing it in a very simple case, and then showing its potential in an interesting example. PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more.\n",
    "\n",
    "**What you will learn today:** In this notebook, we will see how to implement PCA using our old friend SciKitLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1638461060936,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "pvu6v9GfPlxX"
   },
   "outputs": [],
   "source": [
    "#We import th usual packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as sla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsBOk3TJdz8h"
   },
   "source": [
    "## Introducing PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This excercise relies heavily on an example taken from the Python Data Science Handbook, by Jake VanderPlas_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHJRVlPhfJ3S"
   },
   "source": [
    "Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data.\n",
    "\n",
    " Its behavior is easier to visualize by looking at a two-dimensional dataset. \n",
    " \n",
    " Let's consider the following 200 points in a 2D plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1638461061739,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "5NIJK4C7euNg",
    "outputId": "848a7850-575c-4050-b958-95c419940e4a"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0) #initialize seed\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ah-7N4kGn3u-"
   },
   "source": [
    "By eye, it is clear that there is a nearly linear relationship between the x and y variables, but the problem setting here is slightly different from the usual linear regression: rather than attempting to predict the y values from the x values, the unsupervised learning problem attempts to learn about the relationship between the x and y values.\n",
    "\n",
    "In PCA, this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1638461062115,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "0JBA0iv4oO-t",
    "outputId": "b8c56173-38b1-4c27-ab2d-9f90e8131d2f"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vACalhtobtn"
   },
   "source": [
    "Giving the dataset to the function `fit`, the model learns some quantities from the data, most importantly tha *components* and the *explained variance*:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638461062116,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "6EGqrGs2obP_",
    "outputId": "3d725f6f-fc8e-4a17-b389-51de87962964"
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1638461062116,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "C5nLEEVXoYMw",
    "outputId": "d4a0b9d4-2c61-4b9f-eee9-36e7d537fdd3"
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi9qFz06pQf-"
   },
   "source": [
    "To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1638461066437,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "YKCoTJfKoYKG",
    "outputId": "4aed2c83-afad-4e83-928f-78271c1d7852"
   },
   "outputs": [],
   "source": [
    "#to draw a vector going from the point v0 to the point v1\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CY-vXiLrsXup"
   },
   "source": [
    "These vectors represent the principal axes of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the dataâ€”more precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the \"principal components\" of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0R1fWT7EvsOc"
   },
   "source": [
    "## Dimensionality reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZLtj4THvzx7"
   },
   "source": [
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
    "\n",
    "Here is an example of using PCA as a dimensionality reduction transform, using the same data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1638461070610,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "SZ2qpOCwoXtO",
    "outputId": "d9e606ec-5d7f-4a75-96c8-2c0f8a0d4688"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V_24-D1v_8q"
   },
   "source": [
    "The transformed data has been reduced to a single dimension. To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1638461071512,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "cRrKGhC-oXq-",
    "outputId": "1a40310f-f641-4230-d92a-d3515b19ce72"
   },
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2s0K8VZwWck"
   },
   "source": [
    "This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLrqadl18wK1"
   },
   "source": [
    "## PCA as Noise filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXRrxezg82hn"
   },
   "source": [
    "PCA can also be used as a filtering approach for noisy data. The idea is this: any components with variance much larger than the effect of the noise should be relatively unaffected by the noise. So if you reconstruct the data using just the largest subset of principal components, you should be preferentially keeping the signal and throwing out the noise.\n",
    "\n",
    "Let's see how this looks with the digits data. First we will plot several of the input noise-free data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "executionInfo": {
     "elapsed": 1831,
     "status": "ok",
     "timestamp": 1638462138270,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "T3ZyFWWC501J",
    "outputId": "10f44954-365c-4aa3-8c16-83e2e0759dd8"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape\n",
    "\n",
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8, 8),\n",
    "                  cmap='binary', interpolation='nearest',\n",
    "                  clim=(0, 16))\n",
    "plot_digits(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x31mvDIK9Gzc"
   },
   "source": [
    "Now lets add some random noise to create a noisy dataset, and re-plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "executionInfo": {
     "elapsed": 1791,
     "status": "ok",
     "timestamp": 1638462174602,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "Du6lupRf50yy",
    "outputId": "e231d337-19a2-47b1-a059-ebc020ce0937"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "noisy = np.random.normal(digits.data, 4)\n",
    "plot_digits(noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A30sSqxq9UWZ"
   },
   "source": [
    "It's clear by eye that the images are noisy, and contain spurious pixels. Let's train a PCA on the noisy data, requesting that the projection preserve 50% of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1638462222807,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "TH731cZ-9TDO",
    "outputId": "bbe830fe-3ca0-4323-cad0-0ad74d84eaf5"
   },
   "outputs": [],
   "source": [
    "pca = PCA(0.50).fit(noisy)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JdbrTjZ9a_o"
   },
   "source": [
    "Here 50% of the variance amounts to 12 principal components. Now we compute these components, and then use the inverse of the transform to reconstruct the filtered digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "executionInfo": {
     "elapsed": 1695,
     "status": "ok",
     "timestamp": 1638462256693,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "CwmSy0P09cne",
    "outputId": "6f6a2d1e-2be6-41d6-f304-1195f2d04b80"
   },
   "outputs": [],
   "source": [
    "components = pca.transform(noisy)\n",
    "filtered = pca.inverse_transform(components)\n",
    "plot_digits(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gxuGZtZ9noM"
   },
   "source": [
    "A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data. This can be determined by looking at the `cumulative explained variance ratio` as a function of the number of components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1638462375634,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "Ja0W0JuI9clQ",
    "outputId": "86f97805-edfa-4bcd-826a-feefa7457af5"
   },
   "outputs": [],
   "source": [
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWgbZg2s-HR0"
   },
   "source": [
    "This curve quantifies how much of the total, 64-dimensional variance is contained within the first N components. For example, we see that with the digits the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zla7r4w5PcOq"
   },
   "source": [
    "### Your turn: Eigenfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5glzIjBcwmGS"
   },
   "source": [
    "Download the *Labelled Faces in the Wild* (LFW) people dataset from sklearn. This dataset is a collection of pictures of famous people faces. \n",
    "\n",
    "By setting `min_faces_per_person=30`, the extracted dataset will only retain pictures of people that have at least 30 different pictures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1638461072963,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "lIYdn1woOS1n"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1638461073331,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "zZI-GEcoMeYz"
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "\n",
    "min_faces_per_person = 30\n",
    "\n",
    "faces = fetch_lfw_people(min_faces_per_person=min_faces_per_person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3bkDdDSxUA0"
   },
   "source": [
    "Now we plot the first 10 faces (`dataset.target_images`) together with the corresponding names (`dataset.target_names`) using\n",
    "`imshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1708,
     "status": "ok",
     "timestamp": 1638461076355,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "PMyHjCjxNCOQ",
    "outputId": "de0b0029-6406-4594-a882-2c879750998f"
   },
   "outputs": [],
   "source": [
    "# Print the first 10 faces with the corresponding names\n",
    "\n",
    "for X, y in zip(faces.images[:10], faces.target[:10]):\n",
    "    \n",
    "    print(faces.target_names[y])\n",
    "    plt.imshow(X)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1638461077393,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "b6rBmjYuynL-",
    "outputId": "ff0049e0-52eb-487d-911d-011ae50c6abe"
   },
   "outputs": [],
   "source": [
    "print(f\"There are {len(faces.data)} images in total, each composed by {len(faces.data[0])} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks2D4Ey2yI8w"
   },
   "source": [
    "#### 1) In order to obtain a balanced dataset, keep only 30 pictures per person. Then, normalize your data in the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1638461077721,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "mHttd08-1z7i",
    "outputId": "261941a9-73b9-418d-c755-e9063357544b"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "print(f\"Now there are {len(X)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pR7Ay5zFz1tx"
   },
   "source": [
    "#### 2) Do the PCA on the new dataset, compute the principal components and print their shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638461080595,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "w5wswTeARHfT",
    "outputId": "3b998091-72da-473c-cd2c-f177e6e61b7c"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHyOLwbn2Fgt"
   },
   "source": [
    "#### 3) Plot, using `imshow`, the first 5 principal components. How do they look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1638461081482,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "Qg51bX8aROe8",
    "outputId": "bef01a25-b5b7-4d00-ed78-b934b60c796c"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nh3JOldn3GZ_"
   },
   "source": [
    "#### 4) Plot the cumulative explained variance. How many principal components do we need in order to explain the majority of the variance of our faces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1638461081895,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "PCy38BHhUuTH",
    "outputId": "7fdf52a8-9fdf-4493-a2f9-66b0c07861c5"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEj_Lm5T5_gG"
   },
   "source": [
    "## Evaluated Question\n",
    "\n",
    "#### 5) Compute and plot the projections of four pictures on the first {10, 30, 100, 300, 1000} principal components, using the parameter `n_components` and the method `inverse_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7130,
     "status": "ok",
     "timestamp": 1638461446125,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "GdUARuqCVYCd",
    "outputId": "5bd15735-5765-4521-8c64-374dc2907fe4"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkRwyv8n6iYx"
   },
   "source": [
    "### Now, we are going to compare the performance of a naive classifier (the predicted label corresponds to the label of the nearest-neighbour image) when using the full pictures and the first 100 PCA projections.\n",
    "\n",
    "#### 6) First, split your data into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1638461604089,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "gdGy4zOccn7y"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQe0s2_e6y06"
   },
   "source": [
    "#### 7) Use `sklearn.neighbors.KNeighbordClassifier` with `n_neighbors=1` to fit your training set. Evaluate the performance using the method score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1638461604452,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "S5FBOjRpdX0u",
    "outputId": "de115881-6c02-4171-b5a9-e104da4f7c95"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eyb8XK7S7BNQ"
   },
   "source": [
    "#### 8) Now repeat the training but using only the first 100 principal components. Remember to project the test set also before evaluating the score. Compare with the previous score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IHnJGhad6Ih",
    "outputId": "0a1d36ef-308b-4623-b959-c52cfa3b90b0"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsoEdrFM7Ug1"
   },
   "source": [
    "#### 9) When calling `PCA`, set the parameter `whiten=True`. This transformation, called â€˜whiteningâ€™ and it rescales the principal components to have the same variance. Re-classify the faces and compare the accuracy with the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1638461754917,
     "user": {
      "displayName": "davide ghio",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16033550004002351519"
     },
     "user_tz": -60
    },
    "id": "bIoIgF1bhYna",
    "outputId": "bd0a320c-d35e-4aae-b597-533dbe70efb7"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FoIL_TP12.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
